{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR 10 cSAWGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Mm964yyI4AiK",
        "QfUqmsxmTPsu",
        "ltEqQsIpK1Us"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPrgnGl9ZR_E",
        "colab_type": "code",
        "outputId": "40d90d82-8b4c-4205-f904-7b97b3c34e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8xfw1bjrLRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torch.autograd.variable import Variable\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "import PIL\n",
        "from PIL import ImageFont\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from bokeh.io import curdoc, show, output_notebook\n",
        "from bokeh.layouts import column\n",
        "from bokeh.models import ColumnDataSource\n",
        "from bokeh.plotting import figure\n",
        "from functools import partial\n",
        "from threading import Thread\n",
        "from tornado import gen\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import torchvision.utils as vutils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G81dSJ3Uquzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#root folder\n",
        "root_dir=\"/content/drive/My Drive/Fall 2019/Deep Learning CSE 676/Projects/1/DC Gans/\"\n",
        "\n",
        "#data directories\n",
        "#output images\n",
        "output_dir=root_dir+\"output/epoch/\"\n",
        "#input images\n",
        "input_dir=root_dir+\"data/augmented/\"\n",
        "\n",
        "#models\n",
        "model_dir=root_dir+\"model/\"\n",
        "#other resources\n",
        "res_dir=root_dir+\"res/\"\n",
        "#report and logging\n",
        "report_dir=root_dir+\"report/\"\n",
        "\n",
        "\n",
        "#parameters\n",
        "batch_size=100\n",
        "train_split=0.8\n",
        "train_epoch=50\n",
        "\n",
        "#input\n",
        "data_dir=\"data\"\n",
        "inp_width=32\n",
        "inp_height=32\n",
        "inp_channels=3\n",
        "nc=3\n",
        "nz=200\n",
        "\n",
        "#generator\n",
        "ngf=64\n",
        "ndf=64\n",
        "\n",
        "#discriminator\n",
        "ndf=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVDrXwbWfMZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n",
        "\n",
        "def get_parameters():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Model hyper-parameters\n",
        "    parser.add_argument('--model', type=str, default='sagan', choices=['sagan', 'qgan'])\n",
        "    parser.add_argument('--adv_loss', type=str, default='wgan-gp', choices=['wgan-gp', 'hinge'])\n",
        "    parser.add_argument('--imsize', type=int, default=32)\n",
        "    parser.add_argument('--g_num', type=int, default=5)\n",
        "    parser.add_argument('--z_dim', type=int, default=128)\n",
        "    parser.add_argument('--g_conv_dim', type=int, default=64)\n",
        "    parser.add_argument('--d_conv_dim', type=int, default=64)\n",
        "    parser.add_argument('--lambda_gp', type=float, default=10)\n",
        "    parser.add_argument('--version', type=str, default='sagan_1')\n",
        "    \n",
        "    # Training setting\n",
        "    parser.add_argument('--total_step', type=int, default=1000000, help='how many times to update the generator')\n",
        "    parser.add_argument('--d_iters', type=float, default=5)\n",
        "    parser.add_argument('--batch_size', type=int, default=64)\n",
        "    parser.add_argument('--num_workers', type=int, default=2)\n",
        "    parser.add_argument('--g_lr', type=float, default=0.0001)\n",
        "    parser.add_argument('--d_lr', type=float, default=0.0004)\n",
        "    parser.add_argument('--lr_decay', type=float, default=0.95)\n",
        "    parser.add_argument('--beta1', type=float, default=0.0)\n",
        "    parser.add_argument('--beta2', type=float, default=0.9)\n",
        "\n",
        "    # using pretrained\n",
        "    parser.add_argument('--pretrained_model', type=int, default=None)\n",
        "\n",
        "    # Misc\n",
        "    parser.add_argument('--train', type=str2bool, default=True)\n",
        "    parser.add_argument('--parallel', type=str2bool, default=False)\n",
        "    parser.add_argument('--dataset', type=str, default='cifar', choices=['lsun', 'celeb'])\n",
        "    parser.add_argument('--use_tensorboard', type=str2bool, default=False)\n",
        "\n",
        "    # Path\n",
        "    parser.add_argument('--image_path', type=str, default='./data')\n",
        "    parser.add_argument('--log_path', type=str, default='./logs')\n",
        "    parser.add_argument('--model_save_path', type=str, default='./models')\n",
        "    parser.add_argument('--sample_path', type=str, default='./samples')\n",
        "    parser.add_argument('--attn_path', type=str, default='./attn')\n",
        "\n",
        "    # Step size\n",
        "    parser.add_argument('--log_step', type=int, default=10)\n",
        "    parser.add_argument('--sample_step', type=int, default=100)\n",
        "    parser.add_argument('--model_save_step', type=float, default=1.0)\n",
        "\n",
        "\n",
        "    return parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX9_Iv0XfRKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model='sagan'\n",
        "adv_loss='wgan-gp'\n",
        "imsize=64\n",
        "g_num=5\n",
        "z_dim=100\n",
        "label_dim=10\n",
        "g_conv_dim=64\n",
        "d_conv_dim=64\n",
        "lambda_gp=10\n",
        "version='sagan_1'\n",
        "\n",
        "#training parameters\n",
        "total_step=100000\n",
        "d_iters=5\n",
        "batch_size=64\n",
        "num_workers=2\n",
        "g_lr=0.0001\n",
        "d_lr=0.0004\n",
        "lr_decay=0.95\n",
        "beta1=0\n",
        "beta2=0.9\n",
        "\n",
        "#pretrained\n",
        "pretrained_model=None\n",
        "\n",
        "#misc\n",
        "train=True\n",
        "parallel=False\n",
        "dataset='cifar'\n",
        "use_tensorboard=True\n",
        "\n",
        "#paths\n",
        "image_path=\"./data\"\n",
        "log_path=\"./logs\"\n",
        "model_save_path=\"./models\"\n",
        "sample_path=\"./samples\"\n",
        "attn_path=\"./attn\"\n",
        "log_step=10\n",
        "sample_step=100\n",
        "model_save_step=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_7FLsJSq_NS",
        "colab_type": "text"
      },
      "source": [
        "Importing logger to keep a track of training progress "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJAxoFJKhlhW",
        "colab_type": "text"
      },
      "source": [
        "##Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7BZTrX5hn2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class Data_Loader():\n",
        "    def __init__(self, train, dataset, image_path, image_size, batch_size, shuf=True):\n",
        "        self.dataset = dataset\n",
        "        self.path = image_path\n",
        "        self.imsize = image_size\n",
        "        self.batch = batch_size\n",
        "        self.shuf = shuf\n",
        "        self.train = train\n",
        "\n",
        "    def transform(self, resize, totensor, normalize, centercrop):\n",
        "        options = []\n",
        "        if centercrop:\n",
        "            options.append(transforms.CenterCrop(160))\n",
        "        if resize:\n",
        "            options.append(transforms.Resize((self.imsize,self.imsize)))\n",
        "        if totensor:\n",
        "            options.append(transforms.ToTensor())\n",
        "        if normalize:\n",
        "            options.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "        transform = transforms.Compose(options)\n",
        "        return transform\n",
        "\n",
        "    def load_lsun(self, classes='church_outdoor_train'):\n",
        "        transforms = self.transform(True, True, True, False)\n",
        "        dataset = dsets.LSUN(self.path, classes=[classes], transform=transforms)\n",
        "        return dataset\n",
        "\n",
        "    def load_celeb(self):\n",
        "        transforms = self.transform(True, True, True, True)\n",
        "        dataset = dsets.ImageFolder(self.path+'/CelebA', transform=transforms)\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def loader(self):\n",
        "        if self.dataset == 'lsun':\n",
        "            dataset = self.load_lsun()\n",
        "        elif self.dataset == 'celeb':\n",
        "            dataset = self.load_celeb()\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(dataset=self.dataset,\n",
        "                                              batch_size=self.batch,\n",
        "                                              shuffle=self.shuf,\n",
        "                                              num_workers=2,\n",
        "                                              drop_last=True)\n",
        "        return loader\n",
        "      \n",
        "    def loader(self):\n",
        "      transform = transforms.Compose(\n",
        "          [transforms.Resize((self.imsize,self.imsize)),\n",
        "           transforms.ToTensor(),\n",
        "           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "      dataset_full = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                              download=True, transform=transform)\n",
        "      train_split=0.8\n",
        "      train_size=int(train_split*len(dataset_full))\n",
        "      val_size=len(dataset_full)-train_size\n",
        "      trainset, valset=torch.utils.data.random_split(dataset_full,[train_size,val_size])\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "      \n",
        "      return trainloader\n",
        "    def len(self):\n",
        "      return len(self.loader())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1ueGd7hVae",
        "colab_type": "text"
      },
      "source": [
        "##main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWP5I_oUhVHo",
        "colab_type": "code",
        "outputId": "abdf0e4a-db2f-44f1-da7f-06bdc2854a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "from torch.backends import cudnn\n",
        "\n",
        "cudnn.benchmark=True\n",
        "# Data loader\n",
        "data_loader = Data_Loader(train, dataset, image_path, imsize,\n",
        "                        batch_size, shuf=train)\n",
        "\n",
        "\n",
        "# Create directories if not exist\n",
        "make_folder(model_save_path, version)\n",
        "make_folder(sample_path,version)\n",
        "make_folder(log_path,version)\n",
        "make_folder(attn_path,version)\n",
        "\n",
        "\n",
        "if train:\n",
        "    if model=='sagan':\n",
        "        trainer = Trainer(data_loader.loader())\n",
        "    elif model == 'qgan':\n",
        "        trainer = qgan_trainer(data_loader.loader())\n",
        "    trainer.train()\n",
        "else:\n",
        "    tester = Tester(data_loader.loader())\n",
        "    tester.test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Generator(\n",
            "  (l4): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "  )\n",
            "  (l1): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "    )\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "  )\n",
            "  (l2): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "  )\n",
            "  (l3): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "  )\n",
            "  (last): Sequential(\n",
            "    (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): Tanh()\n",
            "  )\n",
            "  (attn1): Self_Attn(\n",
            "    (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (value_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (attn2): Self_Attn(\n",
            "    (query_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (key_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (value_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (l4): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): LeakyReLU(negative_slope=0.1)\n",
            "  )\n",
            "  (l1): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): LeakyReLU(negative_slope=0.1)\n",
            "  )\n",
            "  (l2): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): LeakyReLU(negative_slope=0.1)\n",
            "  )\n",
            "  (l3): Sequential(\n",
            "    (0): SpectralNorm(\n",
            "      (module): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): LeakyReLU(negative_slope=0.1)\n",
            "  )\n",
            "  (last): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
            "  )\n",
            "  (attn1): Self_Attn(\n",
            "    (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (value_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (attn2): Self_Attn(\n",
            "    (query_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (key_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (value_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "tensor(-3.0536, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:05.597805], G_step [10/100000], D_step[10/100000], d_out_real: -3.0536,  ave_gamma_l3: -0.0003, ave_gamma_l4: 0.0011\n",
            "tensor(-10.5100, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:11.145035], G_step [20/100000], D_step[20/100000], d_out_real: -10.5100,  ave_gamma_l3: 0.0000, ave_gamma_l4: 0.0024\n",
            "tensor(-27.8852, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:16.694132], G_step [30/100000], D_step[30/100000], d_out_real: -27.8852,  ave_gamma_l3: 0.0008, ave_gamma_l4: 0.0034\n",
            "tensor(-8.6234, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:22.231616], G_step [40/100000], D_step[40/100000], d_out_real: -8.6234,  ave_gamma_l3: 0.0019, ave_gamma_l4: 0.0048\n",
            "tensor(-9.3373, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:27.754634], G_step [50/100000], D_step[50/100000], d_out_real: -9.3373,  ave_gamma_l3: 0.0030, ave_gamma_l4: 0.0060\n",
            "tensor(-8.9098, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:33.275251], G_step [60/100000], D_step[60/100000], d_out_real: -8.9098,  ave_gamma_l3: 0.0047, ave_gamma_l4: 0.0066\n",
            "tensor(20.7150, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:38.805936], G_step [70/100000], D_step[70/100000], d_out_real: 20.7150,  ave_gamma_l3: 0.0055, ave_gamma_l4: 0.0077\n",
            "tensor(16.7873, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:44.314787], G_step [80/100000], D_step[80/100000], d_out_real: 16.7873,  ave_gamma_l3: 0.0059, ave_gamma_l4: 0.0086\n",
            "tensor(8.6707, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:49.835600], G_step [90/100000], D_step[90/100000], d_out_real: 8.6707,  ave_gamma_l3: 0.0061, ave_gamma_l4: 0.0091\n",
            "tensor(2.9667, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:00:55.354526], G_step [100/100000], D_step[100/100000], d_out_real: 2.9667,  ave_gamma_l3: 0.0063, ave_gamma_l4: 0.0095\n",
            "tensor(7.8423, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:01.006078], G_step [110/100000], D_step[110/100000], d_out_real: 7.8423,  ave_gamma_l3: 0.0063, ave_gamma_l4: 0.0100\n",
            "tensor(-12.8408, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:06.507278], G_step [120/100000], D_step[120/100000], d_out_real: -12.8408,  ave_gamma_l3: 0.0065, ave_gamma_l4: 0.0103\n",
            "tensor(-20.6586, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:12.037965], G_step [130/100000], D_step[130/100000], d_out_real: -20.6586,  ave_gamma_l3: 0.0061, ave_gamma_l4: 0.0107\n",
            "tensor(-17.2324, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:17.566027], G_step [140/100000], D_step[140/100000], d_out_real: -17.2324,  ave_gamma_l3: 0.0052, ave_gamma_l4: 0.0105\n",
            "tensor(-21.0055, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:23.072781], G_step [150/100000], D_step[150/100000], d_out_real: -21.0055,  ave_gamma_l3: 0.0045, ave_gamma_l4: 0.0109\n",
            "tensor(-15.2669, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:28.598055], G_step [160/100000], D_step[160/100000], d_out_real: -15.2669,  ave_gamma_l3: 0.0038, ave_gamma_l4: 0.0106\n",
            "tensor(-16.0820, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:34.121290], G_step [170/100000], D_step[170/100000], d_out_real: -16.0820,  ave_gamma_l3: 0.0034, ave_gamma_l4: 0.0106\n",
            "tensor(-24.8940, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:39.638397], G_step [180/100000], D_step[180/100000], d_out_real: -24.8940,  ave_gamma_l3: 0.0033, ave_gamma_l4: 0.0103\n",
            "tensor(-25.2037, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:45.158492], G_step [190/100000], D_step[190/100000], d_out_real: -25.2037,  ave_gamma_l3: 0.0032, ave_gamma_l4: 0.0105\n",
            "tensor(-32.3818, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:50.662662], G_step [200/100000], D_step[200/100000], d_out_real: -32.3818,  ave_gamma_l3: 0.0031, ave_gamma_l4: 0.0112\n",
            "tensor(-25.1097, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:01:56.276331], G_step [210/100000], D_step[210/100000], d_out_real: -25.1097,  ave_gamma_l3: 0.0034, ave_gamma_l4: 0.0119\n",
            "tensor(-23.9998, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:01.799852], G_step [220/100000], D_step[220/100000], d_out_real: -23.9998,  ave_gamma_l3: 0.0041, ave_gamma_l4: 0.0117\n",
            "tensor(-25.6475, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:07.332619], G_step [230/100000], D_step[230/100000], d_out_real: -25.6475,  ave_gamma_l3: 0.0046, ave_gamma_l4: 0.0119\n",
            "tensor(-15.2058, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:12.837531], G_step [240/100000], D_step[240/100000], d_out_real: -15.2058,  ave_gamma_l3: 0.0051, ave_gamma_l4: 0.0122\n",
            "tensor(-29.1552, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:18.365895], G_step [250/100000], D_step[250/100000], d_out_real: -29.1552,  ave_gamma_l3: 0.0054, ave_gamma_l4: 0.0119\n",
            "tensor(-30.8308, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:23.888411], G_step [260/100000], D_step[260/100000], d_out_real: -30.8308,  ave_gamma_l3: 0.0055, ave_gamma_l4: 0.0116\n",
            "tensor(-29.9974, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:29.404808], G_step [270/100000], D_step[270/100000], d_out_real: -29.9974,  ave_gamma_l3: 0.0056, ave_gamma_l4: 0.0110\n",
            "tensor(-17.4527, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:34.917139], G_step [280/100000], D_step[280/100000], d_out_real: -17.4527,  ave_gamma_l3: 0.0060, ave_gamma_l4: 0.0110\n",
            "tensor(-21.0947, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:40.420861], G_step [290/100000], D_step[290/100000], d_out_real: -21.0947,  ave_gamma_l3: 0.0065, ave_gamma_l4: 0.0107\n",
            "tensor(-25.0719, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:45.932959], G_step [300/100000], D_step[300/100000], d_out_real: -25.0719,  ave_gamma_l3: 0.0068, ave_gamma_l4: 0.0109\n",
            "tensor(-24.4481, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:51.564881], G_step [310/100000], D_step[310/100000], d_out_real: -24.4481,  ave_gamma_l3: 0.0070, ave_gamma_l4: 0.0106\n",
            "tensor(-12.3457, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:02:57.079146], G_step [320/100000], D_step[320/100000], d_out_real: -12.3457,  ave_gamma_l3: 0.0070, ave_gamma_l4: 0.0112\n",
            "tensor(-17.2888, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:02.603673], G_step [330/100000], D_step[330/100000], d_out_real: -17.2888,  ave_gamma_l3: 0.0072, ave_gamma_l4: 0.0108\n",
            "tensor(-9.1441, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:08.126654], G_step [340/100000], D_step[340/100000], d_out_real: -9.1441,  ave_gamma_l3: 0.0076, ave_gamma_l4: 0.0108\n",
            "tensor(-24.7224, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:13.652239], G_step [350/100000], D_step[350/100000], d_out_real: -24.7224,  ave_gamma_l3: 0.0084, ave_gamma_l4: 0.0106\n",
            "tensor(-24.7552, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:19.155875], G_step [360/100000], D_step[360/100000], d_out_real: -24.7552,  ave_gamma_l3: 0.0091, ave_gamma_l4: 0.0106\n",
            "tensor(-35.2143, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:24.677491], G_step [370/100000], D_step[370/100000], d_out_real: -35.2143,  ave_gamma_l3: 0.0098, ave_gamma_l4: 0.0109\n",
            "tensor(-37.0998, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:30.198788], G_step [380/100000], D_step[380/100000], d_out_real: -37.0998,  ave_gamma_l3: 0.0099, ave_gamma_l4: 0.0107\n",
            "tensor(-31.7338, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:35.716780], G_step [390/100000], D_step[390/100000], d_out_real: -31.7338,  ave_gamma_l3: 0.0095, ave_gamma_l4: 0.0111\n",
            "tensor(-42.1915, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:41.227153], G_step [400/100000], D_step[400/100000], d_out_real: -42.1915,  ave_gamma_l3: 0.0101, ave_gamma_l4: 0.0108\n",
            "tensor(-38.4985, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:46.851051], G_step [410/100000], D_step[410/100000], d_out_real: -38.4985,  ave_gamma_l3: 0.0105, ave_gamma_l4: 0.0104\n",
            "tensor(-45.5287, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:52.369091], G_step [420/100000], D_step[420/100000], d_out_real: -45.5287,  ave_gamma_l3: 0.0102, ave_gamma_l4: 0.0109\n",
            "tensor(-53.4034, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:03:57.899672], G_step [430/100000], D_step[430/100000], d_out_real: -53.4034,  ave_gamma_l3: 0.0102, ave_gamma_l4: 0.0105\n",
            "tensor(-56.6801, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:03.402931], G_step [440/100000], D_step[440/100000], d_out_real: -56.6801,  ave_gamma_l3: 0.0099, ave_gamma_l4: 0.0110\n",
            "tensor(-46.5149, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:08.910011], G_step [450/100000], D_step[450/100000], d_out_real: -46.5149,  ave_gamma_l3: 0.0095, ave_gamma_l4: 0.0106\n",
            "tensor(-32.3189, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:14.432874], G_step [460/100000], D_step[460/100000], d_out_real: -32.3189,  ave_gamma_l3: 0.0096, ave_gamma_l4: 0.0106\n",
            "tensor(-42.9527, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:19.953593], G_step [470/100000], D_step[470/100000], d_out_real: -42.9527,  ave_gamma_l3: 0.0097, ave_gamma_l4: 0.0108\n",
            "tensor(-37.5048, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:25.475848], G_step [480/100000], D_step[480/100000], d_out_real: -37.5048,  ave_gamma_l3: 0.0098, ave_gamma_l4: 0.0113\n",
            "tensor(-36.5252, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:30.994174], G_step [490/100000], D_step[490/100000], d_out_real: -36.5252,  ave_gamma_l3: 0.0101, ave_gamma_l4: 0.0111\n",
            "tensor(-44.1325, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:36.514911], G_step [500/100000], D_step[500/100000], d_out_real: -44.1325,  ave_gamma_l3: 0.0101, ave_gamma_l4: 0.0113\n",
            "tensor(-45.6784, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:42.149695], G_step [510/100000], D_step[510/100000], d_out_real: -45.6784,  ave_gamma_l3: 0.0101, ave_gamma_l4: 0.0111\n",
            "tensor(-41.9831, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:47.644418], G_step [520/100000], D_step[520/100000], d_out_real: -41.9831,  ave_gamma_l3: 0.0102, ave_gamma_l4: 0.0114\n",
            "tensor(-43.7880, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:53.160622], G_step [530/100000], D_step[530/100000], d_out_real: -43.7880,  ave_gamma_l3: 0.0109, ave_gamma_l4: 0.0115\n",
            "tensor(-39.4544, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:04:58.676075], G_step [540/100000], D_step[540/100000], d_out_real: -39.4544,  ave_gamma_l3: 0.0110, ave_gamma_l4: 0.0114\n",
            "tensor(-44.7238, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:04.187926], G_step [550/100000], D_step[550/100000], d_out_real: -44.7238,  ave_gamma_l3: 0.0117, ave_gamma_l4: 0.0118\n",
            "tensor(-41.6698, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:09.713983], G_step [560/100000], D_step[560/100000], d_out_real: -41.6698,  ave_gamma_l3: 0.0123, ave_gamma_l4: 0.0114\n",
            "tensor(-46.8921, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:15.236716], G_step [570/100000], D_step[570/100000], d_out_real: -46.8921,  ave_gamma_l3: 0.0130, ave_gamma_l4: 0.0112\n",
            "tensor(-49.7670, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:20.747620], G_step [580/100000], D_step[580/100000], d_out_real: -49.7670,  ave_gamma_l3: 0.0134, ave_gamma_l4: 0.0111\n",
            "tensor(-40.0643, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:26.260835], G_step [590/100000], D_step[590/100000], d_out_real: -40.0643,  ave_gamma_l3: 0.0139, ave_gamma_l4: 0.0108\n",
            "tensor(-48.2494, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:31.776202], G_step [600/100000], D_step[600/100000], d_out_real: -48.2494,  ave_gamma_l3: 0.0141, ave_gamma_l4: 0.0110\n",
            "tensor(-49.8804, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:37.397063], G_step [610/100000], D_step[610/100000], d_out_real: -49.8804,  ave_gamma_l3: 0.0146, ave_gamma_l4: 0.0110\n",
            "tensor(-39.8854, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:42.912935], G_step [620/100000], D_step[620/100000], d_out_real: -39.8854,  ave_gamma_l3: 0.0149, ave_gamma_l4: 0.0112\n",
            "tensor(-26.1528, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:48.656502], G_step [630/100000], D_step[630/100000], d_out_real: -26.1528,  ave_gamma_l3: 0.0149, ave_gamma_l4: 0.0110\n",
            "tensor(-21.9322, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:54.173923], G_step [640/100000], D_step[640/100000], d_out_real: -21.9322,  ave_gamma_l3: 0.0156, ave_gamma_l4: 0.0109\n",
            "tensor(-18.2251, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:05:59.690147], G_step [650/100000], D_step[650/100000], d_out_real: -18.2251,  ave_gamma_l3: 0.0160, ave_gamma_l4: 0.0109\n",
            "tensor(-34.8555, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:05.215837], G_step [660/100000], D_step[660/100000], d_out_real: -34.8555,  ave_gamma_l3: 0.0169, ave_gamma_l4: 0.0116\n",
            "tensor(-24.6071, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:10.743568], G_step [670/100000], D_step[670/100000], d_out_real: -24.6071,  ave_gamma_l3: 0.0169, ave_gamma_l4: 0.0111\n",
            "tensor(-30.5903, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:16.249684], G_step [680/100000], D_step[680/100000], d_out_real: -30.5903,  ave_gamma_l3: 0.0177, ave_gamma_l4: 0.0113\n",
            "tensor(-42.0323, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:21.772038], G_step [690/100000], D_step[690/100000], d_out_real: -42.0323,  ave_gamma_l3: 0.0182, ave_gamma_l4: 0.0107\n",
            "tensor(-35.8923, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:27.272912], G_step [700/100000], D_step[700/100000], d_out_real: -35.8923,  ave_gamma_l3: 0.0187, ave_gamma_l4: 0.0112\n",
            "tensor(-32.2314, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:32.901485], G_step [710/100000], D_step[710/100000], d_out_real: -32.2314,  ave_gamma_l3: 0.0191, ave_gamma_l4: 0.0109\n",
            "tensor(-34.5068, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:38.393970], G_step [720/100000], D_step[720/100000], d_out_real: -34.5068,  ave_gamma_l3: 0.0192, ave_gamma_l4: 0.0114\n",
            "tensor(-33.0063, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:43.906778], G_step [730/100000], D_step[730/100000], d_out_real: -33.0063,  ave_gamma_l3: 0.0197, ave_gamma_l4: 0.0112\n",
            "tensor(-32.8145, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:49.409663], G_step [740/100000], D_step[740/100000], d_out_real: -32.8145,  ave_gamma_l3: 0.0199, ave_gamma_l4: 0.0116\n",
            "tensor(-29.9110, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:06:54.924876], G_step [750/100000], D_step[750/100000], d_out_real: -29.9110,  ave_gamma_l3: 0.0198, ave_gamma_l4: 0.0116\n",
            "tensor(-31.8504, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:00.449930], G_step [760/100000], D_step[760/100000], d_out_real: -31.8504,  ave_gamma_l3: 0.0205, ave_gamma_l4: 0.0118\n",
            "tensor(-26.9740, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:05.958605], G_step [770/100000], D_step[770/100000], d_out_real: -26.9740,  ave_gamma_l3: 0.0213, ave_gamma_l4: 0.0114\n",
            "tensor(-24.9716, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:11.471740], G_step [780/100000], D_step[780/100000], d_out_real: -24.9716,  ave_gamma_l3: 0.0220, ave_gamma_l4: 0.0111\n",
            "tensor(-22.9922, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:16.990551], G_step [790/100000], D_step[790/100000], d_out_real: -22.9922,  ave_gamma_l3: 0.0222, ave_gamma_l4: 0.0111\n",
            "tensor(-24.7194, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:22.512211], G_step [800/100000], D_step[800/100000], d_out_real: -24.7194,  ave_gamma_l3: 0.0224, ave_gamma_l4: 0.0110\n",
            "tensor(-23.8952, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:28.130658], G_step [810/100000], D_step[810/100000], d_out_real: -23.8952,  ave_gamma_l3: 0.0229, ave_gamma_l4: 0.0110\n",
            "tensor(-28.5561, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:33.644858], G_step [820/100000], D_step[820/100000], d_out_real: -28.5561,  ave_gamma_l3: 0.0237, ave_gamma_l4: 0.0107\n",
            "tensor(-30.5195, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:39.170881], G_step [830/100000], D_step[830/100000], d_out_real: -30.5195,  ave_gamma_l3: 0.0243, ave_gamma_l4: 0.0109\n",
            "tensor(-22.6046, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:44.689992], G_step [840/100000], D_step[840/100000], d_out_real: -22.6046,  ave_gamma_l3: 0.0243, ave_gamma_l4: 0.0106\n",
            "tensor(-27.1310, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:50.196821], G_step [850/100000], D_step[850/100000], d_out_real: -27.1310,  ave_gamma_l3: 0.0244, ave_gamma_l4: 0.0108\n",
            "tensor(-27.6263, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:07:55.700637], G_step [860/100000], D_step[860/100000], d_out_real: -27.6263,  ave_gamma_l3: 0.0252, ave_gamma_l4: 0.0113\n",
            "tensor(-26.8616, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:01.225359], G_step [870/100000], D_step[870/100000], d_out_real: -26.8616,  ave_gamma_l3: 0.0252, ave_gamma_l4: 0.0109\n",
            "tensor(-21.4239, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:06.736869], G_step [880/100000], D_step[880/100000], d_out_real: -21.4239,  ave_gamma_l3: 0.0255, ave_gamma_l4: 0.0107\n",
            "tensor(-20.0214, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:12.260722], G_step [890/100000], D_step[890/100000], d_out_real: -20.0214,  ave_gamma_l3: 0.0257, ave_gamma_l4: 0.0108\n",
            "tensor(-27.8621, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:17.792991], G_step [900/100000], D_step[900/100000], d_out_real: -27.8621,  ave_gamma_l3: 0.0263, ave_gamma_l4: 0.0107\n",
            "tensor(-18.4167, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:23.415169], G_step [910/100000], D_step[910/100000], d_out_real: -18.4167,  ave_gamma_l3: 0.0269, ave_gamma_l4: 0.0109\n",
            "tensor(-13.8664, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:28.934687], G_step [920/100000], D_step[920/100000], d_out_real: -13.8664,  ave_gamma_l3: 0.0276, ave_gamma_l4: 0.0109\n",
            "tensor(-14.9247, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:34.461467], G_step [930/100000], D_step[930/100000], d_out_real: -14.9247,  ave_gamma_l3: 0.0279, ave_gamma_l4: 0.0109\n",
            "tensor(-14.6278, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:39.983637], G_step [940/100000], D_step[940/100000], d_out_real: -14.6278,  ave_gamma_l3: 0.0282, ave_gamma_l4: 0.0106\n",
            "tensor(-19.9300, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:45.507062], G_step [950/100000], D_step[950/100000], d_out_real: -19.9300,  ave_gamma_l3: 0.0284, ave_gamma_l4: 0.0105\n",
            "tensor(-29.4565, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:50.981130], G_step [960/100000], D_step[960/100000], d_out_real: -29.4565,  ave_gamma_l3: 0.0289, ave_gamma_l4: 0.0105\n",
            "tensor(-26.0839, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:08:56.509964], G_step [970/100000], D_step[970/100000], d_out_real: -26.0839,  ave_gamma_l3: 0.0292, ave_gamma_l4: 0.0108\n",
            "tensor(-17.0601, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:02.019901], G_step [980/100000], D_step[980/100000], d_out_real: -17.0601,  ave_gamma_l3: 0.0296, ave_gamma_l4: 0.0101\n",
            "tensor(-22.2229, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:07.530185], G_step [990/100000], D_step[990/100000], d_out_real: -22.2229,  ave_gamma_l3: 0.0300, ave_gamma_l4: 0.0104\n",
            "tensor(-22.5507, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:13.038449], G_step [1000/100000], D_step[1000/100000], d_out_real: -22.5507,  ave_gamma_l3: 0.0307, ave_gamma_l4: 0.0101\n",
            "tensor(-11.7382, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:18.655051], G_step [1010/100000], D_step[1010/100000], d_out_real: -11.7382,  ave_gamma_l3: 0.0309, ave_gamma_l4: 0.0104\n",
            "tensor(-16.9629, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:24.160205], G_step [1020/100000], D_step[1020/100000], d_out_real: -16.9629,  ave_gamma_l3: 0.0317, ave_gamma_l4: 0.0101\n",
            "tensor(-20.4693, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:29.680436], G_step [1030/100000], D_step[1030/100000], d_out_real: -20.4693,  ave_gamma_l3: 0.0317, ave_gamma_l4: 0.0102\n",
            "tensor(-19.0729, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:35.188076], G_step [1040/100000], D_step[1040/100000], d_out_real: -19.0729,  ave_gamma_l3: 0.0321, ave_gamma_l4: 0.0104\n",
            "tensor(-14.3840, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:40.703542], G_step [1050/100000], D_step[1050/100000], d_out_real: -14.3840,  ave_gamma_l3: 0.0325, ave_gamma_l4: 0.0105\n",
            "tensor(-21.1622, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:46.223112], G_step [1060/100000], D_step[1060/100000], d_out_real: -21.1622,  ave_gamma_l3: 0.0326, ave_gamma_l4: 0.0101\n",
            "tensor(-24.6449, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:51.733986], G_step [1070/100000], D_step[1070/100000], d_out_real: -24.6449,  ave_gamma_l3: 0.0332, ave_gamma_l4: 0.0104\n",
            "tensor(-22.6739, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:09:57.239352], G_step [1080/100000], D_step[1080/100000], d_out_real: -22.6739,  ave_gamma_l3: 0.0337, ave_gamma_l4: 0.0100\n",
            "tensor(-20.1974, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:02.752496], G_step [1090/100000], D_step[1090/100000], d_out_real: -20.1974,  ave_gamma_l3: 0.0344, ave_gamma_l4: 0.0101\n",
            "tensor(-18.1550, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:08.259119], G_step [1100/100000], D_step[1100/100000], d_out_real: -18.1550,  ave_gamma_l3: 0.0349, ave_gamma_l4: 0.0104\n",
            "tensor(-19.7394, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:13.866957], G_step [1110/100000], D_step[1110/100000], d_out_real: -19.7394,  ave_gamma_l3: 0.0354, ave_gamma_l4: 0.0104\n",
            "tensor(-17.2545, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:19.379336], G_step [1120/100000], D_step[1120/100000], d_out_real: -17.2545,  ave_gamma_l3: 0.0361, ave_gamma_l4: 0.0106\n",
            "tensor(-18.3749, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:24.909774], G_step [1130/100000], D_step[1130/100000], d_out_real: -18.3749,  ave_gamma_l3: 0.0365, ave_gamma_l4: 0.0104\n",
            "tensor(-25.9288, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:30.420573], G_step [1140/100000], D_step[1140/100000], d_out_real: -25.9288,  ave_gamma_l3: 0.0369, ave_gamma_l4: 0.0104\n",
            "tensor(-20.8450, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:35.947718], G_step [1150/100000], D_step[1150/100000], d_out_real: -20.8450,  ave_gamma_l3: 0.0373, ave_gamma_l4: 0.0099\n",
            "tensor(-15.0691, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:41.475438], G_step [1160/100000], D_step[1160/100000], d_out_real: -15.0691,  ave_gamma_l3: 0.0382, ave_gamma_l4: 0.0106\n",
            "tensor(-25.1726, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:47.007159], G_step [1170/100000], D_step[1170/100000], d_out_real: -25.1726,  ave_gamma_l3: 0.0386, ave_gamma_l4: 0.0103\n",
            "tensor(-13.8177, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:52.517629], G_step [1180/100000], D_step[1180/100000], d_out_real: -13.8177,  ave_gamma_l3: 0.0388, ave_gamma_l4: 0.0100\n",
            "tensor(-21.2364, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:10:58.022285], G_step [1190/100000], D_step[1190/100000], d_out_real: -21.2364,  ave_gamma_l3: 0.0389, ave_gamma_l4: 0.0103\n",
            "tensor(-15.2213, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:03.528277], G_step [1200/100000], D_step[1200/100000], d_out_real: -15.2213,  ave_gamma_l3: 0.0389, ave_gamma_l4: 0.0109\n",
            "tensor(-22.0493, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:09.163577], G_step [1210/100000], D_step[1210/100000], d_out_real: -22.0493,  ave_gamma_l3: 0.0391, ave_gamma_l4: 0.0105\n",
            "tensor(-25.4257, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:14.665546], G_step [1220/100000], D_step[1220/100000], d_out_real: -25.4257,  ave_gamma_l3: 0.0394, ave_gamma_l4: 0.0105\n",
            "tensor(-22.0832, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:20.195170], G_step [1230/100000], D_step[1230/100000], d_out_real: -22.0832,  ave_gamma_l3: 0.0397, ave_gamma_l4: 0.0105\n",
            "tensor(-16.5984, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:25.719469], G_step [1240/100000], D_step[1240/100000], d_out_real: -16.5984,  ave_gamma_l3: 0.0395, ave_gamma_l4: 0.0103\n",
            "tensor(-17.0799, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:31.218379], G_step [1250/100000], D_step[1250/100000], d_out_real: -17.0799,  ave_gamma_l3: 0.0403, ave_gamma_l4: 0.0108\n",
            "tensor(-23.3381, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:36.979318], G_step [1260/100000], D_step[1260/100000], d_out_real: -23.3381,  ave_gamma_l3: 0.0404, ave_gamma_l4: 0.0104\n",
            "tensor(-19.3663, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:42.504766], G_step [1270/100000], D_step[1270/100000], d_out_real: -19.3663,  ave_gamma_l3: 0.0404, ave_gamma_l4: 0.0108\n",
            "tensor(-20.9978, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:48.017254], G_step [1280/100000], D_step[1280/100000], d_out_real: -20.9978,  ave_gamma_l3: 0.0410, ave_gamma_l4: 0.0104\n",
            "tensor(-16.0340, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:53.525136], G_step [1290/100000], D_step[1290/100000], d_out_real: -16.0340,  ave_gamma_l3: 0.0413, ave_gamma_l4: 0.0109\n",
            "tensor(-20.6940, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:11:59.048961], G_step [1300/100000], D_step[1300/100000], d_out_real: -20.6940,  ave_gamma_l3: 0.0412, ave_gamma_l4: 0.0103\n",
            "tensor(-17.0176, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:04.669474], G_step [1310/100000], D_step[1310/100000], d_out_real: -17.0176,  ave_gamma_l3: 0.0418, ave_gamma_l4: 0.0108\n",
            "tensor(-19.2597, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:10.169653], G_step [1320/100000], D_step[1320/100000], d_out_real: -19.2597,  ave_gamma_l3: 0.0419, ave_gamma_l4: 0.0106\n",
            "tensor(-11.5595, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:15.701056], G_step [1330/100000], D_step[1330/100000], d_out_real: -11.5595,  ave_gamma_l3: 0.0420, ave_gamma_l4: 0.0109\n",
            "tensor(-15.0426, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:21.220062], G_step [1340/100000], D_step[1340/100000], d_out_real: -15.0426,  ave_gamma_l3: 0.0422, ave_gamma_l4: 0.0109\n",
            "tensor(-15.2356, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:26.741653], G_step [1350/100000], D_step[1350/100000], d_out_real: -15.2356,  ave_gamma_l3: 0.0424, ave_gamma_l4: 0.0106\n",
            "tensor(-19.0914, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:32.264971], G_step [1360/100000], D_step[1360/100000], d_out_real: -19.0914,  ave_gamma_l3: 0.0426, ave_gamma_l4: 0.0109\n",
            "tensor(-19.9649, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:37.793209], G_step [1370/100000], D_step[1370/100000], d_out_real: -19.9649,  ave_gamma_l3: 0.0430, ave_gamma_l4: 0.0105\n",
            "tensor(-18.6075, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:43.310089], G_step [1380/100000], D_step[1380/100000], d_out_real: -18.6075,  ave_gamma_l3: 0.0436, ave_gamma_l4: 0.0107\n",
            "tensor(-18.0934, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:48.830191], G_step [1390/100000], D_step[1390/100000], d_out_real: -18.0934,  ave_gamma_l3: 0.0436, ave_gamma_l4: 0.0103\n",
            "tensor(-19.9209, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:54.344822], G_step [1400/100000], D_step[1400/100000], d_out_real: -19.9209,  ave_gamma_l3: 0.0441, ave_gamma_l4: 0.0103\n",
            "tensor(-11.3355, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:12:59.960395], G_step [1410/100000], D_step[1410/100000], d_out_real: -11.3355,  ave_gamma_l3: 0.0443, ave_gamma_l4: 0.0106\n",
            "tensor(-11.6750, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:05.474595], G_step [1420/100000], D_step[1420/100000], d_out_real: -11.6750,  ave_gamma_l3: 0.0439, ave_gamma_l4: 0.0105\n",
            "tensor(-9.2201, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:10.999859], G_step [1430/100000], D_step[1430/100000], d_out_real: -9.2201,  ave_gamma_l3: 0.0441, ave_gamma_l4: 0.0106\n",
            "tensor(-10.1101, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:16.522170], G_step [1440/100000], D_step[1440/100000], d_out_real: -10.1101,  ave_gamma_l3: 0.0443, ave_gamma_l4: 0.0108\n",
            "tensor(-15.3717, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:22.042448], G_step [1450/100000], D_step[1450/100000], d_out_real: -15.3717,  ave_gamma_l3: 0.0446, ave_gamma_l4: 0.0106\n",
            "tensor(-21.7356, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:27.564342], G_step [1460/100000], D_step[1460/100000], d_out_real: -21.7356,  ave_gamma_l3: 0.0446, ave_gamma_l4: 0.0102\n",
            "tensor(-15.1200, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:33.096087], G_step [1470/100000], D_step[1470/100000], d_out_real: -15.1200,  ave_gamma_l3: 0.0447, ave_gamma_l4: 0.0104\n",
            "tensor(-15.6329, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:38.621452], G_step [1480/100000], D_step[1480/100000], d_out_real: -15.6329,  ave_gamma_l3: 0.0450, ave_gamma_l4: 0.0107\n",
            "tensor(-11.3556, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:44.140824], G_step [1490/100000], D_step[1490/100000], d_out_real: -11.3556,  ave_gamma_l3: 0.0450, ave_gamma_l4: 0.0107\n",
            "tensor(-12.3371, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:49.665297], G_step [1500/100000], D_step[1500/100000], d_out_real: -12.3371,  ave_gamma_l3: 0.0454, ave_gamma_l4: 0.0108\n",
            "tensor(-10.8903, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:13:55.291930], G_step [1510/100000], D_step[1510/100000], d_out_real: -10.8903,  ave_gamma_l3: 0.0451, ave_gamma_l4: 0.0103\n",
            "tensor(-9.1554, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:00.816865], G_step [1520/100000], D_step[1520/100000], d_out_real: -9.1554,  ave_gamma_l3: 0.0453, ave_gamma_l4: 0.0103\n",
            "tensor(-13.1244, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:06.340644], G_step [1530/100000], D_step[1530/100000], d_out_real: -13.1244,  ave_gamma_l3: 0.0453, ave_gamma_l4: 0.0107\n",
            "tensor(-19.1092, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:11.853772], G_step [1540/100000], D_step[1540/100000], d_out_real: -19.1092,  ave_gamma_l3: 0.0455, ave_gamma_l4: 0.0100\n",
            "tensor(-8.2128, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:17.363043], G_step [1550/100000], D_step[1550/100000], d_out_real: -8.2128,  ave_gamma_l3: 0.0459, ave_gamma_l4: 0.0107\n",
            "tensor(-13.5316, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:22.881144], G_step [1560/100000], D_step[1560/100000], d_out_real: -13.5316,  ave_gamma_l3: 0.0458, ave_gamma_l4: 0.0102\n",
            "tensor(-10.8938, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:28.390891], G_step [1570/100000], D_step[1570/100000], d_out_real: -10.8938,  ave_gamma_l3: 0.0460, ave_gamma_l4: 0.0107\n",
            "tensor(-6.9451, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:33.914673], G_step [1580/100000], D_step[1580/100000], d_out_real: -6.9451,  ave_gamma_l3: 0.0463, ave_gamma_l4: 0.0109\n",
            "tensor(-10.0040, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:39.431771], G_step [1590/100000], D_step[1590/100000], d_out_real: -10.0040,  ave_gamma_l3: 0.0466, ave_gamma_l4: 0.0102\n",
            "tensor(-9.2928, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:44.955791], G_step [1600/100000], D_step[1600/100000], d_out_real: -9.2928,  ave_gamma_l3: 0.0464, ave_gamma_l4: 0.0104\n",
            "tensor(-9.4643, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:50.592388], G_step [1610/100000], D_step[1610/100000], d_out_real: -9.4643,  ave_gamma_l3: 0.0468, ave_gamma_l4: 0.0105\n",
            "tensor(-14.0891, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:14:56.105333], G_step [1620/100000], D_step[1620/100000], d_out_real: -14.0891,  ave_gamma_l3: 0.0467, ave_gamma_l4: 0.0101\n",
            "tensor(-6.6226, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:01.617604], G_step [1630/100000], D_step[1630/100000], d_out_real: -6.6226,  ave_gamma_l3: 0.0469, ave_gamma_l4: 0.0105\n",
            "tensor(-11.3381, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:07.142653], G_step [1640/100000], D_step[1640/100000], d_out_real: -11.3381,  ave_gamma_l3: 0.0471, ave_gamma_l4: 0.0103\n",
            "tensor(-15.7111, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:12.672793], G_step [1650/100000], D_step[1650/100000], d_out_real: -15.7111,  ave_gamma_l3: 0.0472, ave_gamma_l4: 0.0104\n",
            "tensor(-13.3473, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:18.204951], G_step [1660/100000], D_step[1660/100000], d_out_real: -13.3473,  ave_gamma_l3: 0.0474, ave_gamma_l4: 0.0101\n",
            "tensor(-7.9443, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:23.728363], G_step [1670/100000], D_step[1670/100000], d_out_real: -7.9443,  ave_gamma_l3: 0.0475, ave_gamma_l4: 0.0106\n",
            "tensor(-11.3385, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:29.253723], G_step [1680/100000], D_step[1680/100000], d_out_real: -11.3385,  ave_gamma_l3: 0.0477, ave_gamma_l4: 0.0100\n",
            "tensor(-11.9713, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:34.769492], G_step [1690/100000], D_step[1690/100000], d_out_real: -11.9713,  ave_gamma_l3: 0.0478, ave_gamma_l4: 0.0105\n",
            "tensor(-16.0699, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:40.299268], G_step [1700/100000], D_step[1700/100000], d_out_real: -16.0699,  ave_gamma_l3: 0.0482, ave_gamma_l4: 0.0105\n",
            "tensor(-18.1982, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:45.942703], G_step [1710/100000], D_step[1710/100000], d_out_real: -18.1982,  ave_gamma_l3: 0.0481, ave_gamma_l4: 0.0101\n",
            "tensor(-15.0184, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:51.463341], G_step [1720/100000], D_step[1720/100000], d_out_real: -15.0184,  ave_gamma_l3: 0.0483, ave_gamma_l4: 0.0099\n",
            "tensor(-11.6372, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:15:56.988026], G_step [1730/100000], D_step[1730/100000], d_out_real: -11.6372,  ave_gamma_l3: 0.0484, ave_gamma_l4: 0.0099\n",
            "tensor(-12.9254, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:02.505846], G_step [1740/100000], D_step[1740/100000], d_out_real: -12.9254,  ave_gamma_l3: 0.0484, ave_gamma_l4: 0.0102\n",
            "tensor(-10.4514, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:08.026042], G_step [1750/100000], D_step[1750/100000], d_out_real: -10.4514,  ave_gamma_l3: 0.0483, ave_gamma_l4: 0.0103\n",
            "tensor(-6.8955, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:13.550751], G_step [1760/100000], D_step[1760/100000], d_out_real: -6.8955,  ave_gamma_l3: 0.0486, ave_gamma_l4: 0.0103\n",
            "tensor(-16.9322, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:19.040744], G_step [1770/100000], D_step[1770/100000], d_out_real: -16.9322,  ave_gamma_l3: 0.0488, ave_gamma_l4: 0.0103\n",
            "tensor(-9.0690, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:24.538437], G_step [1780/100000], D_step[1780/100000], d_out_real: -9.0690,  ave_gamma_l3: 0.0486, ave_gamma_l4: 0.0105\n",
            "tensor(-9.7977, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:30.058269], G_step [1790/100000], D_step[1790/100000], d_out_real: -9.7977,  ave_gamma_l3: 0.0488, ave_gamma_l4: 0.0108\n",
            "tensor(-8.7194, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:35.583074], G_step [1800/100000], D_step[1800/100000], d_out_real: -8.7194,  ave_gamma_l3: 0.0491, ave_gamma_l4: 0.0103\n",
            "tensor(-10.4328, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:41.229418], G_step [1810/100000], D_step[1810/100000], d_out_real: -10.4328,  ave_gamma_l3: 0.0493, ave_gamma_l4: 0.0104\n",
            "tensor(-6.7022, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:46.741452], G_step [1820/100000], D_step[1820/100000], d_out_real: -6.7022,  ave_gamma_l3: 0.0490, ave_gamma_l4: 0.0107\n",
            "tensor(-6.3670, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:52.267780], G_step [1830/100000], D_step[1830/100000], d_out_real: -6.3670,  ave_gamma_l3: 0.0492, ave_gamma_l4: 0.0108\n",
            "tensor(-6.2346, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:16:57.774428], G_step [1840/100000], D_step[1840/100000], d_out_real: -6.2346,  ave_gamma_l3: 0.0490, ave_gamma_l4: 0.0103\n",
            "tensor(-9.0496, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:03.285861], G_step [1850/100000], D_step[1850/100000], d_out_real: -9.0496,  ave_gamma_l3: 0.0492, ave_gamma_l4: 0.0105\n",
            "tensor(-3.1178, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:08.789671], G_step [1860/100000], D_step[1860/100000], d_out_real: -3.1178,  ave_gamma_l3: 0.0494, ave_gamma_l4: 0.0107\n",
            "tensor(-8.4511, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:14.309947], G_step [1870/100000], D_step[1870/100000], d_out_real: -8.4511,  ave_gamma_l3: 0.0495, ave_gamma_l4: 0.0103\n",
            "tensor(-3.7881, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:20.050884], G_step [1880/100000], D_step[1880/100000], d_out_real: -3.7881,  ave_gamma_l3: 0.0496, ave_gamma_l4: 0.0110\n",
            "tensor(-7.3686, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:25.563906], G_step [1890/100000], D_step[1890/100000], d_out_real: -7.3686,  ave_gamma_l3: 0.0498, ave_gamma_l4: 0.0109\n",
            "tensor(-2.8464, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:31.090325], G_step [1900/100000], D_step[1900/100000], d_out_real: -2.8464,  ave_gamma_l3: 0.0495, ave_gamma_l4: 0.0107\n",
            "tensor(-4.8262, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:36.706509], G_step [1910/100000], D_step[1910/100000], d_out_real: -4.8262,  ave_gamma_l3: 0.0495, ave_gamma_l4: 0.0109\n",
            "tensor(-0.7956, device='cuda:0', grad_fn=<NegBackward>)\n",
            "Elapsed [0:17:42.227555], G_step [1920/100000], D_step[1920/100000], d_out_real: -0.7956,  ave_gamma_l3: 0.0498, ave_gamma_l4: 0.0105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-5da1ba338dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'qgan'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqgan_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtester\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-d7d5bf3b1f95>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# Compute loss with real images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# dr1, dr2, df1, df2, gf1, gf2 are attention scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor2var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0md_out_real\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madv_loss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'wgan-gp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c347e3f9198e>\u001b[0m in \u001b[0;36mtensor2var\u001b[0;34m(x, grad)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensor2var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmZ9gfjUjs_A",
        "colab_type": "text"
      },
      "source": [
        "##Spectral\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eknZPG2jjv_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLzOWR3EjUs3",
        "colab_type": "text"
      },
      "source": [
        "##models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiXGEDQ1jWwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "        \n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature \n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N) \n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "        \n",
        "        out = self.gamma*out + x\n",
        "        return out,attention\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, image_size=64, z_dim=100, label_dim=10,conv_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        repeat_num = int(np.log2(self.imsize)) - 3\n",
        "        mult = 2 ** repeat_num # 8\n",
        "        layer11.append(SpectralNorm(nn.ConvTranspose2d(z_dim, (conv_dim * mult)/2, 4)))\n",
        "        layer11.append(nn.BatchNorm2d(conv_dim * mult))\n",
        "        layer11.append(nn.ReLU())\n",
        "        \n",
        "        layer12.append(SpectralNorm(nn.ConvTranspose2d(label_dim, (conv_dim * mult)/2, 4)))\n",
        "        layer12.append(nn.BatchNorm2d(conv_dim * mult))\n",
        "        layer12.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = conv_dim * mult\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer2.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer3.append(nn.ReLU())\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "            layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "            layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "            layer4.append(nn.ReLU())\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        self.l11 = nn.Sequential(*layer11)\n",
        "        self.l12 = nn.Sequential(*layer12)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n",
        "        last.append(nn.Tanh())\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn( 128, 'relu')\n",
        "        self.attn2 = Self_Attn( 64,  'relu')\n",
        "\n",
        "    def forward(self, z,label):\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        label = label.view(label.size(0), label.size(1), 1, 1)\n",
        "        out_z=self.l11(z)\n",
        "        out_label=self.l12(label)\n",
        "        out=torch.cat([out_z,out_label],1)\n",
        "        out=self.l2(out)\n",
        "        out=self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out, p1, p2\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Discriminator, Auxiliary Classifier.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=64, image_size=64, conv_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        layer1.append(SpectralNorm(nn.Conv2d(3, (conv_dim)/2, 4, 2, 1)))\n",
        "        layer1.append(nn.LeakyReLU(0.1))\n",
        "        layer1.append(SpectralNorm(nn.Conv2d(1, (conv_dim)/2, 4, 2, 1)))\n",
        "        layer1.append(nn.LeakyReLU(0.1))\n",
        "\n",
        "        curr_dim = conv_dim\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer2.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer3.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "            layer4.append(nn.LeakyReLU(0.1))\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = curr_dim*2\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.Conv2d(curr_dim, 1, 4))\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn(256, 'relu')\n",
        "        self.attn2 = Self_Attn(512, 'relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.l2(out)\n",
        "        out = self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out.squeeze(), p1, p2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvbTgHvxkImH",
        "colab_type": "text"
      },
      "source": [
        "##utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBtgYjx4kJ6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def make_folder(path, version):\n",
        "        if not os.path.exists(os.path.join(path, version)):\n",
        "            os.makedirs(os.path.join(path, version))\n",
        "\n",
        "\n",
        "def tensor2var(x, grad=False):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x, requires_grad=grad)\n",
        "\n",
        "def var2tensor(x):\n",
        "    return x.data.cpu()\n",
        "\n",
        "def var2numpy(x):\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp_(0, 1)\n",
        "  \n",
        "def encode(label):\n",
        "  res=[0]*10\n",
        "  labels={'airplane':0,\t\t\t\t\t\t\t\t\t\t\n",
        "          'automobile':1,\t\t\t\t\t\t\t\t\t\t\n",
        "          'bird':2,\t\t\t\t\t\t\t\t\t\t\n",
        "          'cat':3,\t\t\t\t\t\t\t\t\t\t\n",
        "          'deer':4,\t\t\t\t\t\t\t\t\t\t\n",
        "          'dog':5,\t\t\t\t\t\t\t\t\t\t\n",
        "          'frog':6,\t\t\t\t\t\t\t\t\t\t\n",
        "          'horse':7,\t\t\t\t\t\t\t\t\t\t\n",
        "          'ship':8,\t\t\t\t\t\t\t\t\t\t\n",
        "          'truck':9}\n",
        "  res[labels[label]]=1\n",
        "  return torch.Tensor(res)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAdfdhZTjMRf",
        "colab_type": "text"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Vy5roAjK5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import datetime\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, data_loader):\n",
        "\n",
        "        # Data loader\n",
        "        self.data_loader = data_loader\n",
        "\n",
        "        # exact model and loss\n",
        "        self.model = model\n",
        "        self.adv_loss = adv_loss\n",
        "\n",
        "        # Model hyper-parameters\n",
        "        self.imsize = imsize\n",
        "        self.g_num = g_num\n",
        "        self.z_dim = z_dim\n",
        "        self.g_conv_dim = g_conv_dim\n",
        "        self.d_conv_dim = d_conv_dim\n",
        "        self.parallel = parallel\n",
        "\n",
        "        self.lambda_gp = lambda_gp\n",
        "        self.total_step = total_step\n",
        "        self.d_iters = d_iters\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.g_lr = g_lr\n",
        "        self.d_lr = d_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.pretrained_model = pretrained_model\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.use_tensorboard = use_tensorboard\n",
        "        self.image_path = image_path\n",
        "        self.log_path = log_path\n",
        "        self.model_save_path = model_save_path\n",
        "        self.sample_path = sample_path\n",
        "        self.log_step = log_step\n",
        "        self.sample_step = sample_step\n",
        "        self.model_save_step = model_save_step\n",
        "        self.version = version\n",
        "\n",
        "        # Path\n",
        "        self.log_path = os.path.join(log_path, self.version)\n",
        "        self.sample_path = os.path.join(sample_path, self.version)\n",
        "        self.model_save_path = os.path.join(model_save_path, self.version)\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            self.build_tensorboard()\n",
        "\n",
        "        # Start with trained model\n",
        "        if self.pretrained_model:\n",
        "            self.load_pretrained_model()\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # Data iterator\n",
        "        data_iter = iter(self.data_loader)\n",
        "        step_per_epoch = len(self.data_loader)\n",
        "        model_save_step = int(self.model_save_step * step_per_epoch)\n",
        "\n",
        "        # Fixed input for debugging\n",
        "        fixed_z = tensor2var(torch.randn(self.batch_size, self.z_dim))\n",
        "\n",
        "        # Start with trained model\n",
        "        if self.pretrained_model:\n",
        "            start = self.pretrained_model + 1\n",
        "        else:\n",
        "            start = 0\n",
        "\n",
        "        # Start time\n",
        "        start_time = time.time()\n",
        "        for step in range(start, self.total_step):\n",
        "\n",
        "            # ================== Train D ================== #\n",
        "            self.D.train()\n",
        "            self.G.train()\n",
        "\n",
        "            try:\n",
        "                real_images, _ = next(data_iter)\n",
        "            except:\n",
        "                data_iter = iter(self.data_loader)\n",
        "                real_images, _ = next(data_iter)\n",
        "\n",
        "            # Compute loss with real images\n",
        "            # dr1, dr2, df1, df2, gf1, gf2 are attention scores\n",
        "            real_images = tensor2var(real_images)\n",
        "            d_out_real,dr1,dr2 = self.D(real_images)\n",
        "            if self.adv_loss == 'wgan-gp':\n",
        "                d_loss_real = - torch.mean(d_out_real)\n",
        "            elif self.adv_loss == 'hinge':\n",
        "                d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\n",
        "\n",
        "            # apply Gumbel Softmax\n",
        "            z = tensor2var(torch.randn(real_images.size(0), self.z_dim))\n",
        "            fake_images,gf1,gf2 = self.G(z)\n",
        "            d_out_fake,df1,df2 = self.D(fake_images)\n",
        "\n",
        "            if self.adv_loss == 'wgan-gp':\n",
        "                d_loss_fake = d_out_fake.mean()\n",
        "            elif self.adv_loss == 'hinge':\n",
        "                d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\n",
        "\n",
        "\n",
        "            # Backward + Optimize\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "            self.reset_grad()\n",
        "            d_loss.backward()\n",
        "            self.d_optimizer.step()\n",
        "\n",
        "\n",
        "            if self.adv_loss == 'wgan-gp':\n",
        "                # Compute gradient penalty\n",
        "                alpha = torch.rand(real_images.size(0), 1, 1, 1).cuda().expand_as(real_images)\n",
        "                interpolated = Variable(alpha * real_images.data + (1 - alpha) * fake_images.data, requires_grad=True)\n",
        "                out,_,_ = self.D(interpolated)\n",
        "\n",
        "                grad = torch.autograd.grad(outputs=out,\n",
        "                                           inputs=interpolated,\n",
        "                                           grad_outputs=torch.ones(out.size()).cuda(),\n",
        "                                           retain_graph=True,\n",
        "                                           create_graph=True,\n",
        "                                           only_inputs=True)[0]\n",
        "\n",
        "                grad = grad.view(grad.size(0), -1)\n",
        "                grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
        "                d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
        "\n",
        "                # Backward + Optimize\n",
        "                d_loss = self.lambda_gp * d_loss_gp\n",
        "\n",
        "                self.reset_grad()\n",
        "                d_loss.backward()\n",
        "                self.d_optimizer.step()\n",
        "\n",
        "            # ================== Train G and gumbel ================== #\n",
        "            # Create random noise\n",
        "            z = tensor2var(torch.randn(real_images.size(0), self.z_dim))\n",
        "            fake_images,_,_ = self.G(z)\n",
        "\n",
        "            # Compute loss with fake images\n",
        "            g_out_fake,_,_ = self.D(fake_images)  # batch x n\n",
        "            if self.adv_loss == 'wgan-gp':\n",
        "                g_loss_fake = - g_out_fake.mean()\n",
        "            elif self.adv_loss == 'hinge':\n",
        "                g_loss_fake = - g_out_fake.mean()\n",
        "\n",
        "            self.reset_grad()\n",
        "            g_loss_fake.backward()\n",
        "            self.g_optimizer.step()\n",
        "\n",
        "\n",
        "            # Print out log info\n",
        "            if (step + 1) % self.log_step == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "                print(d_loss_real)\n",
        "                print(\"Elapsed [{}], G_step [{}/{}], D_step[{}/{}], d_out_real: {:.4f}, \"\n",
        "                      \" ave_gamma_l3: {:.4f}, ave_gamma_l4: {:.4f}\".\n",
        "                      format(elapsed, step + 1, self.total_step, (step + 1),\n",
        "                             self.total_step , d_loss_real.item(),\n",
        "                             self.G.attn1.gamma.mean().item(), self.G.attn2.gamma.mean().item()))\n",
        "\n",
        "            # Sample images\n",
        "            if (step + 1) % self.sample_step == 0:\n",
        "                fake_images,_,_= self.G(fixed_z)\n",
        "                save_image(denorm(fake_images.data),\n",
        "                           os.path.join(self.sample_path, '{}_fake.png'.format(step + 1)))\n",
        "\n",
        "            if (step+1) % model_save_step==0:\n",
        "                torch.save(self.G.state_dict(),\n",
        "                           os.path.join(self.model_save_path, '{}_G.pth'.format(step + 1)))\n",
        "                torch.save(self.D.state_dict(),\n",
        "                           os.path.join(self.model_save_path, '{}_D.pth'.format(step + 1)))\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        self.G = Generator(self.batch_size,self.imsize, self.z_dim, self.g_conv_dim).cuda()\n",
        "        self.D = Discriminator(self.batch_size,self.imsize, self.d_conv_dim).cuda()\n",
        "        if self.parallel:\n",
        "            self.G = nn.DataParallel(self.G)\n",
        "            self.D = nn.DataParallel(self.D)\n",
        "\n",
        "        # Loss and optimizer\n",
        "        # self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "        self.g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.G.parameters()), self.g_lr, [self.beta1, self.beta2])\n",
        "        self.d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.D.parameters()), self.d_lr, [self.beta1, self.beta2])\n",
        "\n",
        "        self.c_loss = torch.nn.CrossEntropyLoss()\n",
        "        # print networks\n",
        "        print(self.G)\n",
        "        print(self.D)\n",
        "\n",
        "    def build_tensorboard(self):\n",
        "        return\n",
        "        #from logger import Logger\n",
        "        #self.logger = Logger(self.log_path)\n",
        "\n",
        "    def load_pretrained_model(self):\n",
        "        self.G.load_state_dict(torch.load(os.path.join(\n",
        "            self.model_save_path, '{}_G.pth'.format(self.pretrained_model))))\n",
        "        self.D.load_state_dict(torch.load(os.path.join(\n",
        "            self.model_save_path, '{}_D.pth'.format(self.pretrained_model))))\n",
        "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n",
        "\n",
        "    def reset_grad(self):\n",
        "        self.d_optimizer.zero_grad()\n",
        "        self.g_optimizer.zero_grad()\n",
        "\n",
        "    def save_sample(self, data_iter):\n",
        "        real_images, _ = next(data_iter)\n",
        "        save_image(denorm(real_images), os.path.join(self.sample_path, 'real.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm964yyI4AiK",
        "colab_type": "text"
      },
      "source": [
        "##utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6pzQjPG4B2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import itertools, imageio, torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "#from scipy.misc import imresize\n",
        "#import PIL\n",
        "#import pilutil\n",
        "\n",
        "\n",
        "def ones_target(size):\n",
        "  #returning tensor variable containing ones, with shape =size\n",
        "  data=Variable(torch.ones(size,1))\n",
        "  return data\n",
        "\n",
        "def zeros_target(size):\n",
        "  data=Variable(torch.zeros(size,1))\n",
        "  return data\n",
        "\n",
        "def images_to_vectors(images):\n",
        "  return images.view(images.size(0),3072) #parameters: (size to flatten(28 here, row size), no. of images to be flattened)\n",
        "\n",
        "def vectors_to_images(vectors):\n",
        "  return vectors.view(vectors.size(0),3,32,32) # to convert each row of vectors to 28x28 pixels images \n",
        "# parameteres: (initial flattened dim, no. of sub vectors to be converted to 2d, 2d dimensions of output )\n",
        "\n",
        "def show_result(G, x_, num_epoch, show = False, save = False, path = 'result.png'):\n",
        "    # G.eval()\n",
        "    y_ = G(x_)\n",
        "    y_=y_[:4]\n",
        "    y_ = y_.cpu()\n",
        "\n",
        "    size_figure_grid = 3\n",
        "    fig, ax = plt.subplots(x_.size()[0], size_figure_grid, figsize=(5, 5))\n",
        "    for i, j in itertools.product(range(x_.size()[0]), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for i in range(y_.size()[0],2):\n",
        "\n",
        "        ax[i, 0].cla()\n",
        "        ax[i, 0].imshow((y_[i].numpy().transpose(1, 2, 0) + 1) / 2)\n",
        "        ax[i, 1].cla()\n",
        "        ax[i, 1].imshow((y_[i+1].numpy().transpose(1, 2, 0) + 1) / 2)\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Iter')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def generate_animation(root, model, opt):\n",
        "    images = []\n",
        "    for e in range(opt.train_epoch):\n",
        "        img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
        "        images.append(imageio.imread(img_name))\n",
        "    imageio.mimsave(root + model + 'generate_animation.gif', images, fps=5)\n",
        "\n",
        "def data_load(path, subfolder, transform, batch_size, shuffle=True):\n",
        "    dset = datasets.ImageFolder(path, transform)\n",
        "    ind = dset.class_to_idx[subfolder]\n",
        "\n",
        "    n = 0\n",
        "    for i in range(dset.__len__()):\n",
        "        if ind != dset.imgs[n][1]:\n",
        "            del dset.imgs[n]\n",
        "            n -= 1\n",
        "\n",
        "        n += 1\n",
        "\n",
        "    return torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def imgs_resize(imgs, resize_scale = 286):\n",
        "    outputs = torch.FloatTensor(imgs.size()[0], imgs.size()[1], resize_scale, resize_scale)\n",
        "    for i in range(imgs.size()[0]):\n",
        "        img = imresize(imgs[i].numpy(), [resize_scale, resize_scale])\n",
        "        outputs[i] = torch.FloatTensor((img.transpose(2, 0, 1).astype(np.float32).reshape(-1, imgs.size()[1], resize_scale, resize_scale) - 127.5) / 127.5)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "def random_crop(imgs1, imgs2, crop_size = 256):\n",
        "    outputs1 = torch.FloatTensor(imgs1.size()[0], imgs1.size()[1], crop_size, crop_size)\n",
        "    outputs2 = torch.FloatTensor(imgs2.size()[0], imgs2.size()[1], crop_size, crop_size)\n",
        "    for i in range(imgs1.size()[0]):\n",
        "        img1 = imgs1[i]\n",
        "        img2 = imgs2[i]\n",
        "        rand1 = np.random.randint(0, imgs1.size()[2] - crop_size)\n",
        "        rand2 = np.random.randint(0, imgs2.size()[2] - crop_size)\n",
        "        outputs1[i] = img1[:, rand1: crop_size + rand1, rand2: crop_size + rand2]\n",
        "        outputs2[i] = img2[:, rand1: crop_size + rand1, rand2: crop_size + rand2]\n",
        "\n",
        "    return outputs1, outputs2\n",
        "\n",
        "def random_fliplr(imgs1, imgs2):\n",
        "    outputs1 = torch.FloatTensor(imgs1.size())\n",
        "    outputs2 = torch.FloatTensor(imgs2.size())\n",
        "    for i in range(imgs1.size()[0]):\n",
        "        if torch.rand(1)[0] < 0.5:\n",
        "            img1 = torch.FloatTensor(\n",
        "                (np.fliplr(imgs1[i].numpy().transpose(1, 2, 0)).transpose(2, 0, 1).reshape(-1, imgs1.size()[1], imgs1.size()[2], imgs1.size()[3]) + 1) / 2)\n",
        "            outputs1[i] = (img1 - 0.5) / 0.5\n",
        "            img2 = torch.FloatTensor(\n",
        "                (np.fliplr(imgs2[i].numpy().transpose(1, 2, 0)).transpose(2, 0, 1).reshape(-1, imgs2.size()[1], imgs2.size()[2], imgs2.size()[3]) + 1) / 2)\n",
        "            outputs2[i] = (img2 - 0.5) / 0.5\n",
        "        else:\n",
        "            outputs1[i] = imgs1[i]\n",
        "            outputs2[i] = imgs2[i]\n",
        "\n",
        "    return outputs1, outputs2\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "    \n",
        "def noise(size):\n",
        "  n=Variable(torch.randn(size,200))\n",
        "  return n\n",
        " \n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8xtngMv2yDK",
        "colab_type": "text"
      },
      "source": [
        "##Loading the CIFAR Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahwUvnwh20hC",
        "colab_type": "code",
        "outputId": "6b485a07-6b94-4af6-bbcb-dc1bd7806ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "train_size=int(train_split*len(dataset))\n",
        "val_size=len(dataset)-train_size\n",
        "trainset, valset=torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(\"Size of train set\",len(trainloader))\n",
        "print(\"Size of val set\",len(valloader))\n",
        "print(\"Size of test set\",len(testloader))\n",
        "\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n",
        "print(\"Size of images\",images.size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Size of train set 400\n",
            "Size of val set 100\n",
            "Size of test set 2500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAAD8CAYAAADT2P50AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " bird   car  frog  ship plane  frog truck   car   dog truck   dog   cat horse  deer  frog  frog  bird  bird  frog  deer truck   cat  ship  ship  ship   cat   dog   cat  deer  deer   car   car horse horse  frog horse  bird  bird   cat  ship   cat  bird  bird  ship   car  ship horse  ship  deer horse horse truck  frog  deer   dog   dog horse  frog  bird   dog   car   dog horse  ship truck plane plane   dog   dog  bird horse  frog plane  deer plane   dog   dog   car   dog plane  bird  bird  bird   dog   cat plane   car   car   cat horse plane  deer plane  bird truck truck  bird truck horse   car\n",
            "Size of images torch.Size([100, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to1Jq4QK1fzh",
        "colab_type": "text"
      },
      "source": [
        "Adding funcionality to flatten an image and vice versa. \n",
        "Flattening means to convert a multidimensional matrix to one dimensional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfUqmsxmTPsu",
        "colab_type": "text"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMAU28spIIyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(200, ngf * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(    ngf,      3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "            \n",
        "    def forward(self, input):\n",
        "#         if input.is_cuda and self.ngpu > 1:\n",
        "#             output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "#         else:\n",
        "#             output = self.main(input)\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 1, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "#         if input.is_cuda and self.ngpu > 1:\n",
        "#             output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "#         else:\n",
        "#             output = self.main(input)\n",
        "        output = self.main(input)\n",
        "        return output.view(-1, 1).squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WpEN-sGKBOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_test_samples=16\n",
        "#generating test samples\n",
        "test_noise=noise(num_test_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltEqQsIpK1Us",
        "colab_type": "text"
      },
      "source": [
        "##Training the Gan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUHXgR9MOwdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "def register_extension(id, extension): Image.EXTENSION[extension.lower()] = id.upper()\n",
        "Image.register_extension = register_extension\n",
        "def register_extensions(id, extensions): \n",
        "  for extension in extensions: register_extension(id, extension)\n",
        "Image.register_extensions = register_extensions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfTDecmxK_Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "#creating logger instance\n",
        "num_epochs=50\n",
        "num_batches = len(trainloader)\n",
        "for epoch in range(num_epochs):\n",
        "  for n_batch,(real_batch,_) in enumerate(trainloader):\n",
        "    N=real_batch.size(0)\n",
        "    \n",
        "    #Train discriminator\n",
        "    real_data=Variable(images_to_vectors(real_batch))\n",
        "    #generate fake data and detach to avoid further calculation of gradients\n",
        "    fake_data=generator(noise(N)).detach()\n",
        "    \n",
        "    #train\n",
        "    d_error,d_pred_real,d_pred_fake=train_discriminator(d_optimizer, real_data, fake_data)\n",
        "    \n",
        "    #Train Generator\n",
        "    #generate fake data\n",
        "    fake_data=generator(noise(N))\n",
        "    #train \n",
        "    g_error=train_generator(g_optimizer, fake_data)\n",
        "    #Log batch error\n",
        "    \n",
        "    if(n_batch)%100==0: #??\n",
        "      test_images=vectors_to_images(generator(test_noise))\n",
        "      test_images=test_images.data\n",
        "            \n",
        "      #display status logs\n",
        "      import matplotlib.pyplot as plt\n",
        "      f, axarr = plt.subplots(2, 2)\n",
        "      axarr[0, 0].imshow(test_images[0,0])\n",
        "      axarr[0,1].imshow(test_images[1,0])\n",
        "      axarr[1, 0].imshow(test_images[2,0])\n",
        "      axarr[1, 1].imshow(test_images[3,0])\n",
        "      # Fine-tune figure; make subplots farther from each other.\n",
        "      f.subplots_adjust(hspace=0.3)\n",
        "\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIS_HptgayEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(2, 2)\n",
        "axarr[0, 0].imshow(test_images[0,0])\n",
        "axarr[0,1].imshow(test_images[1,0])\n",
        "axarr[1, 0].imshow(test_images[2,0])\n",
        "axarr[1, 1].imshow(test_images[3,0])\n",
        "# Fine-tune figure; make subplots farther from each other.\n",
        "f.subplots_adjust(hspace=0.3)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiM3SlQkAVt0",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkHirAiOOJ05",
        "colab_type": "code",
        "outputId": "096299d6-574e-41f6-d851-51e5a9e146f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHL5-Ns5zg1O",
        "colab_type": "text"
      },
      "source": [
        "##From pix2pix model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjeSuVBhQVUO",
        "colab_type": "code",
        "outputId": "bb3557fd-7c78-418a-aad1-82b9b889d4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "print(len(dataset))\n",
        "train_size=int(train_split*len(dataset))\n",
        "val_size=len(dataset)-train_size\n",
        "train_dataset, val_dataset=torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "train_dataloader=torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                              batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                          num_workers=4)\n",
        "num_batches=len(train_dataloader)\n",
        "val_dataloader=torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                          num_workers=4)\n",
        "\n",
        "\n",
        "#from model import generator, discriminator\n",
        "#import utils\n",
        "\n",
        "#parameters\n",
        "lrG=0.02\n",
        "lrD=0.02\n",
        "beta1=0.5\n",
        "beta2=0.999\n",
        "L1_lambda=1.5\n",
        "\n",
        "\n",
        "\n",
        "start_time=time.time()\n",
        "epoch_start=0\n",
        "epoch_end=epoch_start+train_epoch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = Generator(ngf)\n",
        "D = Discriminator(ndf)\n",
        "G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "#loss\n",
        "# During discriminator forward-backward-update\n",
        "D_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n",
        "# During generator forward-backward-update\n",
        "G_loss = -torch.mean(D_fake)\n",
        "BCE_loss=nn.BCELoss().to(device)\n",
        "L1_loss=nn.L1Loss().to(device)\n",
        "\n",
        "#Loading the model if previously exists\n",
        "if(os.path.isfile(model_dir+'generator_param.pkl') and os.path.isfile(model_dir+'discriminator_param.pkl')):\n",
        "\n",
        "  G_checkpoint=torch.load(model_dir+'generator_param.pkl',map_location=device)\n",
        "  D_checkpoint=torch.load(model_dir+'discriminator_param.pkl',map_location=device)\n",
        "  G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "  D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "  #D.eval()\n",
        "\n",
        "  G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "  D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  train_hist=G_checkpoint['train_hist']\n",
        "  epoch_start=G_checkpoint['epoch']\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "#Esle creating new model\n",
        "else:  \n",
        "  print(\"Previous model not found. Restarting train process...\")\n",
        "  G.apply(weights_init)\n",
        "  D.apply(weights_init)\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "\n",
        "\n",
        "  G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "  D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "  train_hist={}\n",
        "  train_hist['D_losses']=[]\n",
        "  train_hist['G_losses']=[]\n",
        "  train_hist['per_epoch_ptimes']=[]\n",
        "  train_hist['total_ptime']=[]\n",
        "  epoch_start=0\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "\n",
        "#training\n",
        "for epoch in range(epoch_start,epoch_end):\n",
        "  D_losses=[]\n",
        "  G_losses=[]\n",
        "  epoch_start_time=time.time()\n",
        "  num_iter=0\n",
        "  for (real_image, _) in train_dataloader:\n",
        "    inp_noise=torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    inp_noise,real_image=Variable(inp_noise.to(device)),Variable(real_image.to(device))\n",
        "    D.zero_grad()\n",
        "\n",
        "    D_result=D(real_image).squeeze()\n",
        "    \n",
        "    D_real_loss=BCE_loss(D_result,Variable(torch.ones(D_result.size()).to(device)))\n",
        "    \n",
        "    G_result=G(inp_noise)\n",
        "    G_result=G_result.detach()\n",
        "    D_result=D(G_result).squeeze()\n",
        "    D_fake_loss=BCE_loss(D_result,Variable(torch.zeros(D_result.size()).to(device)))\n",
        "    \n",
        "    D_train_loss=(D_real_loss +D_fake_loss)*0.5\n",
        "    D_train_loss.backward()\n",
        "    D_optimizer.step()\n",
        "    train_hist['D_losses'].append(float(D_train_loss))\n",
        "\n",
        "    D_losses.append(float(D_train_loss))\n",
        "    D_losses.append(float(0))\n",
        "\n",
        "    #training generator\n",
        "    G.zero_grad()\n",
        "\n",
        "    G_result=G(inp_noise)\n",
        "    G_result=G_result.detach()\n",
        "    D_result=D(G_result).squeeze()\n",
        "    G_train_loss=BCE_loss(D_result, Variable(torch.ones(D_result.size()).to(device))) + L1_lambda*L1_loss(G_result,real_image)\n",
        "    G_train_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    train_hist['G_losses'].append(float(G_train_loss))\n",
        "    G_losses.append(float(G_train_loss))\n",
        "    num_iter+=1\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            'train_hist': train_hist\n",
        "            }, model_dir+'generator_param.pkl')\n",
        "\n",
        "  torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            },model_dir+'discriminator_param.pkl')\n",
        "\n",
        "  epoch_end_time=time.time()\n",
        "  per_epoch_ptime=epoch_end_time-epoch_start_time\n",
        "  print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                              torch.mean(torch.FloatTensor(G_losses))))\n",
        "  fixed_p =  output_dir  + str(epoch + 1) + '.png'\n",
        "  save_noise=torch.randn(1, nz, 1, 1, device=device)\n",
        "  #show_result(G, Variable(inp_noise.to(device), volatile=True), real_image.cpu(), (epoch+1), save=True, path=fixed_p)\n",
        "  vutils.save_image(G(save_noise).detach(),\n",
        "                    fixed_p,\n",
        "                    normalize=True)\n",
        "  train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "end_time=time.time()\n",
        "total_ptime=end_time-start_time\n",
        "train_hist['total_ptime'].append(total_ptime)\n",
        "print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), train_epoch, total_ptime))\n",
        "\n",
        "\n",
        "with open(report_dir+'train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path=report_dir + 'train_hist.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "50000\n",
            "Previous model not found. Restarting train process...\n",
            "[1/50] - ptime: 16.26, loss_d: 0.029, loss_g: 5.731\n",
            "[2/50] - ptime: 16.07, loss_d: 0.001, loss_g: 7.907\n",
            "[3/50] - ptime: 16.17, loss_d: 0.000, loss_g: 8.986\n",
            "[4/50] - ptime: 16.09, loss_d: 0.000, loss_g: 9.727\n",
            "[5/50] - ptime: 16.09, loss_d: 0.000, loss_g: 10.012\n",
            "[6/50] - ptime: 16.14, loss_d: 0.000, loss_g: 10.585\n",
            "[7/50] - ptime: 16.11, loss_d: 0.000, loss_g: 11.140\n",
            "[8/50] - ptime: 16.14, loss_d: 0.000, loss_g: 11.441\n",
            "[9/50] - ptime: 16.18, loss_d: 0.000, loss_g: 11.627\n",
            "[10/50] - ptime: 15.96, loss_d: 0.000, loss_g: 12.038\n",
            "[11/50] - ptime: 16.06, loss_d: 0.000, loss_g: 12.381\n",
            "[12/50] - ptime: 16.03, loss_d: 0.000, loss_g: 12.450\n",
            "[13/50] - ptime: 16.14, loss_d: 0.019, loss_g: 9.880\n",
            "[14/50] - ptime: 16.04, loss_d: 0.000, loss_g: 8.651\n",
            "[15/50] - ptime: 16.18, loss_d: 0.000, loss_g: 9.545\n",
            "[16/50] - ptime: 16.04, loss_d: 0.000, loss_g: 10.025\n",
            "[17/50] - ptime: 16.13, loss_d: 0.000, loss_g: 10.500\n",
            "[18/50] - ptime: 16.16, loss_d: 0.000, loss_g: 10.862\n",
            "[19/50] - ptime: 16.20, loss_d: 0.000, loss_g: 11.202\n",
            "[20/50] - ptime: 16.26, loss_d: 0.000, loss_g: 11.515\n",
            "[21/50] - ptime: 16.18, loss_d: 0.000, loss_g: 11.796\n",
            "[22/50] - ptime: 16.18, loss_d: 0.000, loss_g: 12.072\n",
            "[23/50] - ptime: 16.21, loss_d: 0.000, loss_g: 12.327\n",
            "[24/50] - ptime: 16.13, loss_d: 0.000, loss_g: 12.565\n",
            "[25/50] - ptime: 16.22, loss_d: 0.000, loss_g: 12.813\n",
            "[26/50] - ptime: 16.14, loss_d: 0.000, loss_g: 13.081\n",
            "[27/50] - ptime: 16.16, loss_d: 0.000, loss_g: 13.305\n",
            "[28/50] - ptime: 16.20, loss_d: 0.000, loss_g: 13.561\n",
            "[29/50] - ptime: 16.13, loss_d: 0.000, loss_g: 13.754\n",
            "[30/50] - ptime: 16.18, loss_d: 0.000, loss_g: 14.004\n",
            "[31/50] - ptime: 16.10, loss_d: 0.000, loss_g: 14.207\n",
            "[32/50] - ptime: 16.30, loss_d: 0.000, loss_g: 14.442\n",
            "[33/50] - ptime: 16.23, loss_d: 0.000, loss_g: 14.665\n",
            "[34/50] - ptime: 16.23, loss_d: 0.000, loss_g: 14.889\n",
            "[35/50] - ptime: 16.21, loss_d: 0.000, loss_g: 15.126\n",
            "[36/50] - ptime: 16.19, loss_d: 0.000, loss_g: 15.321\n",
            "[37/50] - ptime: 16.21, loss_d: 0.000, loss_g: 15.447\n",
            "[38/50] - ptime: 16.20, loss_d: 0.000, loss_g: 15.774\n",
            "[39/50] - ptime: 16.27, loss_d: 0.000, loss_g: 16.001\n",
            "[40/50] - ptime: 16.19, loss_d: 0.000, loss_g: 16.256\n",
            "[41/50] - ptime: 16.18, loss_d: 0.000, loss_g: 16.505\n",
            "[42/50] - ptime: 16.18, loss_d: 0.000, loss_g: 16.692\n",
            "[43/50] - ptime: 16.22, loss_d: 0.000, loss_g: 16.928\n",
            "[44/50] - ptime: 16.19, loss_d: 0.000, loss_g: 17.209\n",
            "[45/50] - ptime: 16.27, loss_d: 0.000, loss_g: 17.364\n",
            "[46/50] - ptime: 16.18, loss_d: 0.000, loss_g: 17.570\n",
            "[47/50] - ptime: 16.28, loss_d: 0.000, loss_g: 17.818\n",
            "[48/50] - ptime: 16.22, loss_d: 0.000, loss_g: 18.017\n",
            "[49/50] - ptime: 16.14, loss_d: 0.000, loss_g: 18.238\n",
            "[50/50] - ptime: 16.24, loss_d: 0.000, loss_g: 18.471\n",
            "Avg one epoch ptime: 16.17, total 50 epochs ptime: 808.81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-8de3bcf62228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train_hist.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Fall 2019/Deep Learning CSE 676/Projects/1/DC Gans/report/train_hist.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ2N_xo549TJ",
        "colab_type": "text"
      },
      "source": [
        "## W GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEdPsLJnBCax",
        "colab_type": "code",
        "outputId": "f67004db-21b2-49c8-e6e6-81447ac6b1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "print(len(dataset))\n",
        "train_size=int(train_split*len(dataset))\n",
        "val_size=len(dataset)-train_size\n",
        "train_dataset, val_dataset=torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "train_dataloader=torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                              batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                          num_workers=4)\n",
        "num_batches=len(train_dataloader)\n",
        "val_dataloader=torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                          num_workers=4)\n",
        "\n",
        "\n",
        "#from model import generator, discriminator\n",
        "#import utils\n",
        "\n",
        "#parameters\n",
        "lrG=0.02\n",
        "lrD=0.02\n",
        "beta1=0.5\n",
        "beta2=0.999\n",
        "L1_lambda=1.5\n",
        "\n",
        "\n",
        "\n",
        "start_time=time.time()\n",
        "epoch_start=0\n",
        "epoch_end=epoch_start+train_epoch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = Generator(ngf)\n",
        "D = Discriminator(ndf)\n",
        "G_optimizer=torch.optim.RMSprop(G.parameters(), lr=0.0001)\n",
        "D_optimizer=torch.optim.RMSprop(D.parameters(), lr=0.0001)\n",
        "\n",
        "BCE_loss=nn.BCELoss().to(device)\n",
        "L1_loss=nn.L1Loss().to(device)\n",
        "\n",
        "#Loading the model if previously exists\n",
        "if(os.path.isfile(model_dir+'generator_param.pkl') and os.path.isfile(model_dir+'discriminator_param.pkl')):\n",
        "\n",
        "  G_checkpoint=torch.load(model_dir+'generator_param.pkl',map_location=device)\n",
        "  D_checkpoint=torch.load(model_dir+'discriminator_param.pkl',map_location=device)\n",
        "  G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "  D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "  #D.eval()\n",
        "\n",
        "  G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "  D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  train_hist=G_checkpoint['train_hist']\n",
        "  epoch_start=G_checkpoint['epoch']\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "#Esle creating new model\n",
        "else:  \n",
        "  print(\"Previous model not found. Restarting train process...\")\n",
        "  G.apply(weights_init)\n",
        "  D.apply(weights_init)\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "\n",
        "\n",
        "  G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "  D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "  train_hist={}\n",
        "  train_hist['D_losses']=[]\n",
        "  train_hist['G_losses']=[]\n",
        "  train_hist['per_epoch_ptimes']=[]\n",
        "  train_hist['total_ptime']=[]\n",
        "  epoch_start=0\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "\n",
        "#training\n",
        "for epoch in range(epoch_start,epoch_end):\n",
        "  D_losses=[]\n",
        "  G_losses=[]\n",
        "  epoch_start_time=time.time()\n",
        "  num_iter=0\n",
        "  for (real_image, _) in train_dataloader:\n",
        "    inp_noise=torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    inp_noise,real_image=Variable(inp_noise.to(device)),Variable(real_image.to(device))\n",
        "    D.zero_grad()\n",
        "\n",
        "    D_real=D(real_image).squeeze()\n",
        "\n",
        "    \n",
        "    G_result=G(inp_noise)\n",
        "    G_result=G_result.detach()\n",
        "    D_fake=D(G_result).squeeze()\n",
        "\n",
        "    D_train_loss=-torch.mean(D_real) + torch.mean(D_fake)\n",
        "    D_train_loss.backward()\n",
        "    D_optimizer.step()\n",
        "    \n",
        "    for p in D.parameters():\n",
        "      p.data.clamp_(-0.01, 0.01)\n",
        "    \n",
        "    train_hist['D_losses'].append(float(D_train_loss))\n",
        "\n",
        "    D_losses.append(float(D_train_loss))\n",
        "    D_losses.append(float(0))\n",
        "    for _ in range(5):\n",
        "      #training generator\n",
        "      G.zero_grad()\n",
        "\n",
        "      G_result=G(inp_noise)\n",
        "      G_result=G_result.detach()\n",
        "      D_result=D(G_result).squeeze()\n",
        "      G_train_loss = -torch.mean(D_result)\n",
        "      G_train_loss.backward()\n",
        "      G_optimizer.step()\n",
        "\n",
        "    train_hist['G_losses'].append(float(G_train_loss))\n",
        "    G_losses.append(float(G_train_loss))\n",
        "    num_iter+=1\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            'train_hist': train_hist\n",
        "            }, model_dir+'generator_param.pkl')\n",
        "\n",
        "  torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            },model_dir+'discriminator_param.pkl')\n",
        "\n",
        "  epoch_end_time=time.time()\n",
        "  per_epoch_ptime=epoch_end_time-epoch_start_time\n",
        "  print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                              torch.mean(torch.FloatTensor(G_losses))))\n",
        "  fixed_p =  output_dir  + str(epoch + 1) + '.png'\n",
        "  save_noise=torch.randn(1, nz, 1, 1, device=device)\n",
        "  #show_result(G, Variable(inp_noise.to(device), volatile=True), real_image.cpu(), (epoch+1), save=True, path=fixed_p)\n",
        "  vutils.save_image(G(save_noise).detach(),\n",
        "                    fixed_p,\n",
        "                    normalize=True)\n",
        "  train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "end_time=time.time()\n",
        "total_ptime=end_time-start_time\n",
        "train_hist['total_ptime'].append(total_ptime)\n",
        "print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), train_epoch, total_ptime))\n",
        "\n",
        "\n",
        "with open(report_dir+'train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path=report_dir + 'train_hist.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-7d456cb1b2d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mD_train_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mD_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grad_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0msquare_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'square_avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'square_avg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-r-Wk7i4_aH",
        "colab_type": "text"
      },
      "source": [
        "##FID Score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRMIiBZF5jth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "try:\n",
        "    from torchvision.models.utils import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "# Inception weights ported to Pytorch from\n",
        "# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
        "FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'\n",
        "\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False,\n",
        "                 use_fid_inception=True):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        if use_fid_inception:\n",
        "            inception = fid_inception_v3()\n",
        "        else:\n",
        "            inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "\n",
        "\n",
        "def fid_inception_v3():\n",
        "    \"\"\"Build pretrained Inception model for FID computation\n",
        "    The Inception model for FID computation uses a different set of weights\n",
        "    and has a slightly different structure than torchvision's Inception.\n",
        "    This method first constructs torchvision's Inception and then patches the\n",
        "    necessary parts that are different in the FID Inception model.\n",
        "    \"\"\"\n",
        "    inception = models.inception_v3(num_classes=1008,\n",
        "                                    aux_logits=False,\n",
        "                                    pretrained=False)\n",
        "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
        "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
        "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
        "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
        "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
        "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
        "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
        "\n",
        "    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n",
        "    inception.load_state_dict(state_dict)\n",
        "    return inception\n",
        "\n",
        "\n",
        "class FIDInceptionA(models.inception.InceptionA):\n",
        "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionC(models.inception.InceptionC):\n",
        "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_1(models.inception.InceptionE):\n",
        "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_2(models.inception.InceptionE):\n",
        "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: The FID Inception model uses max pooling instead of average\n",
        "        # pooling. This is likely an error in this specific Inception\n",
        "        # implementation, as other Inception models use average pooling here\n",
        "        # (which matches the description in the paper).\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpXgYycu5CeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  \n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import linalg\n",
        "from scipy.misc import imread\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    # If not tqdm is not available, provide a mock version of it\n",
        "    def tqdm(x): return x\n",
        "\n",
        "#from inception import InceptionV3\n",
        "'''\n",
        "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
        "parser.add_argument('path', type=str, nargs=2,\n",
        "                    help=('Path to the generated images or '\n",
        "                          'to .npz statistic files'))\n",
        "parser.add_argument('--batch-size', type=int, default=50,\n",
        "                    help='Batch size to use')\n",
        "parser.add_argument('--dims', type=int, default=2048,\n",
        "                    choices=list(InceptionV3.BLOCK_INDEX_BY_DIM),\n",
        "                    help=('Dimensionality of Inception features to use. '\n",
        "                          'By default, uses pool3 features'))\n",
        "parser.add_argument('-c', '--gpu', default='', type=str,\n",
        "                    help='GPU to use (leave blank for CPU only)')\n",
        "\n",
        "'''\n",
        "def get_activations(files, model, batch_size=50, dims=2048,\n",
        "                    cuda=False, verbose=False):\n",
        "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
        "    Params:\n",
        "    -- files       : List of image files paths\n",
        "    -- model       : Instance of inception model\n",
        "    -- batch_size  : Batch size of images for the model to process at once.\n",
        "                     Make sure that the number of samples is a multiple of\n",
        "                     the batch size, otherwise some samples are ignored. This\n",
        "                     behavior is retained to match the original FID score\n",
        "                     implementation.\n",
        "    -- dims        : Dimensionality of features returned by Inception\n",
        "    -- cuda        : If set to True, use GPU\n",
        "    -- verbose     : If set to True and parameter out_step is given, the number\n",
        "                     of calculated batches is reported.\n",
        "    Returns:\n",
        "    -- A numpy array of dimension (num images, dims) that contains the\n",
        "       activations of the given tensor when feeding inception with the\n",
        "       query tensor.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if len(files) % batch_size != 0:\n",
        "        print(('Warning: number of images is not a multiple of the '\n",
        "               'batch size. Some samples are going to be ignored.'))\n",
        "    if batch_size > len(files):\n",
        "        print(('Warning: batch size is bigger than the data size. '\n",
        "               'Setting batch size to data size'))\n",
        "        batch_size = len(files)\n",
        "\n",
        "    n_batches = len(files) // batch_size\n",
        "    n_used_imgs = n_batches * batch_size\n",
        "\n",
        "    pred_arr = np.empty((n_used_imgs, dims))\n",
        "\n",
        "    for i in tqdm(range(n_batches)):\n",
        "        if verbose:\n",
        "            print('\\rPropagating batch %d/%d' % (i + 1, n_batches),\n",
        "                  end='', flush=True)\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "\n",
        "        images = np.array([imread(str(f)).astype(np.float32)\n",
        "                           for f in files[start:end]])\n",
        "\n",
        "        # Reshape to (n_images, 3, height, width)\n",
        "        images = images.transpose((0, 3, 1, 2))\n",
        "        images /= 255\n",
        "\n",
        "        batch = torch.from_numpy(images).type(torch.FloatTensor)\n",
        "        if cuda:\n",
        "            batch = batch.cuda()\n",
        "\n",
        "        pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "        if pred.shape[2] != 1 or pred.shape[3] != 1:\n",
        "            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n",
        "\n",
        "    if verbose:\n",
        "        print(' done')\n",
        "\n",
        "    return pred_arr\n",
        "\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    Stable version by Dougal J. Sutherland.\n",
        "    Params:\n",
        "    -- mu1   : Numpy array containing the activations of a layer of the\n",
        "               inception net (like returned by the function 'get_predictions')\n",
        "               for generated samples.\n",
        "    -- mu2   : The sample mean over activations, precalculated on an\n",
        "               representative data set.\n",
        "    -- sigma1: The covariance matrix over activations for generated samples.\n",
        "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "               representative data set.\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "\n",
        "def calculate_activation_statistics(files, model, batch_size=50,\n",
        "                                    dims=2048, cuda=False, verbose=False):\n",
        "    \"\"\"Calculation of the statistics used by the FID.\n",
        "    Params:\n",
        "    -- files       : List of image files paths\n",
        "    -- model       : Instance of inception model\n",
        "    -- batch_size  : The images numpy array is split into batches with\n",
        "                     batch size batch_size. A reasonable batch size\n",
        "                     depends on the hardware.\n",
        "    -- dims        : Dimensionality of features returned by Inception\n",
        "    -- cuda        : If set to True, use GPU\n",
        "    -- verbose     : If set to True and parameter out_step is given, the\n",
        "                     number of calculated batches is reported.\n",
        "    Returns:\n",
        "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
        "               the inception model.\n",
        "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
        "               the inception model.\n",
        "    \"\"\"\n",
        "    act = get_activations(files, model, batch_size, dims, cuda, verbose)\n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "\n",
        "def _compute_statistics_of_path(path, model, batch_size, dims, cuda):\n",
        "    if path.endswith('.npz'):\n",
        "        f = np.load(path)\n",
        "        m, s = f['mu'][:], f['sigma'][:]\n",
        "        f.close()\n",
        "    else:\n",
        "        path = pathlib.Path(path)\n",
        "        files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
        "        m, s = calculate_activation_statistics(files, model, batch_size,\n",
        "                                               dims, cuda)\n",
        "\n",
        "    return m, s\n",
        "\n",
        "\n",
        "def calculate_fid_given_paths(paths, batch_size, cuda, dims):\n",
        "    \"\"\"Calculates the FID of two paths\"\"\"\n",
        "    for p in paths:\n",
        "        if not os.path.exists(p):\n",
        "            raise RuntimeError('Invalid path: %s' % p)\n",
        "\n",
        "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
        "\n",
        "    model = InceptionV3([block_idx])\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    m1, s1 = _compute_statistics_of_path(paths[0], model, batch_size,\n",
        "                                         dims, cuda)\n",
        "    m2, s2 = _compute_statistics_of_path(paths[1], model, batch_size,\n",
        "                                         dims, cuda)\n",
        "    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
        "\n",
        "    return fid_value\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "\n",
        "    fid_value = calculate_fid_given_paths(args.path,\n",
        "                                          args.batch_size,\n",
        "                                          args.gpu != '',\n",
        "                                          args.dims)\n",
        "    print('FID: ', fid_value)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}